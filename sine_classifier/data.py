#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
data.py
=======
ç«¯åˆ°ç«¯SINE Dataset - 100%å¯¹é½å®˜æ–¹tokenizerç‰ˆæœ¬ï¼ˆå·²ä¿®å¤å¯¹é½é—®é¢˜ï¼‰

å…³é”®æ”¹è¿›ï¼š
1. å®Œå…¨ä½¿ç”¨å®˜æ–¹ tokenizer æ„å»º token-base æ˜ å°„ï¼ˆdecode single tokenï¼‰
2. è‡ªåŠ¨å¤„ç†è¶…é•¿åºåˆ—ï¼Œä¿æŒ 3' ç«¯ï¼ˆå·¦æˆªæ–­ token çº§åˆ«ï¼‰
3. CLS token ä½¿ç”¨ meanï¼ˆæ›´åˆç†ï¼‰
4. padding ä½ç½®å¼ºåˆ¶ä¸º 0.0
5. å»é™¤äº†æ‰€æœ‰æ‰‹åŠ¨ k-mer é€»è¾‘ï¼Œæœç»å¯¹é½è¯¯å·®
"""

import numpy as np
import pandas as pd
import torch
from torch.utils.data import Dataset
from typing import List, Optional, Tuple

def gaussian_kernel_smooth(mask: np.ndarray, sigma: float = 3.0) -> np.ndarray:
    """
    å¯¹ä¸€ç»´æ•°ç»„åº”ç”¨é«˜æ–¯å¹³æ»‘
    æ¨¡æ‹Ÿ Motif ä¿¡å·çš„ç”Ÿç‰©å­¦æ¸å˜ç‰¹å¾
    """
    if sigma <= 0:
        return mask
    
    # åˆ›å»ºé«˜æ–¯æ ¸ (æ ¸å¤§å° = 6*sigma + 1ï¼Œä¿è¯è¦†ç›–ç»å¤§éƒ¨åˆ†æ¦‚ç‡)
    radius = int(3 * sigma)
    x = np.arange(-radius, radius + 1)
    kernel = np.exp(-x**2 / (2 * sigma**2))
    kernel = kernel / kernel.sum()  # å½’ä¸€åŒ–
    
    # å·ç§¯å¹³æ»‘ (mode='same' ä¿æŒé•¿åº¦ä¸å˜)
    smoothed = np.convolve(mask, kernel, mode='same')
    return smoothed
    
class SINEDatasetE2E(Dataset):
    def __init__(
        self,
        sequences_with_ids: List[Tuple[str, str]],
        labels: Optional[List[int]],
        motif_df: pd.DataFrame,
        tokenizer,
        max_token_length: int = 1024
    ):
        self.sequences_with_ids = sequences_with_ids
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_token_length = max_token_length
        self.is_training = labels is not None
        
        self.motif_data = motif_df.set_index('unique_id', drop=False)
        
        if self.is_training:
            assert len(sequences_with_ids) == len(labels)

        self.cls_token_id = tokenizer.cls_token_id
        self.pad_token_id = tokenizer.pad_token_id
        
        # [é…ç½®] åŠ¨æ€æƒé‡çš„èŒƒå›´ (min, max)
        # è®­ç»ƒæ—¶åœ¨è¿™ä¸ªèŒƒå›´å†…éšæœºé‡‡æ ·ï¼Œé¢„æµ‹æ—¶å–å‡å€¼
        self.weight_ranges = {
            'A_box': (1.8, 2.4),    # æ ¸å¿ƒç‰¹å¾ï¼Œæƒé‡æœ€é«˜
            'B_box': (1.8, 2.4),
            'polyA': (1.4, 1.8),    # æ¬¡è¦ç‰¹å¾
            'left_TSD': (0.8, 1.2), # è¾…åŠ©ç‰¹å¾
            'right_TSD': (0.8, 1.2),
            'background': (0.2, 0.4) # èƒŒæ™¯æå‡åˆ° 0.3 å·¦å³ï¼Œé¿å…è¿‡åº¦æŠ‘åˆ¶
        }
        
        print(f"[INFO] SINE Dataset åŠ è½½å®Œæˆ (Mask Dropout=0.7, Smoothing=ON)")

    def __len__(self):
        return len(self.sequences_with_ids)

    def __getitem__(self, idx):
        unique_id, raw_sequence = self.sequences_with_ids[idx]
        label = self.labels[idx] if self.is_training else None
        
        sequence = raw_sequence.upper().replace('U', 'T')
        seq_len = len(sequence)
        
        # 1. åºåˆ—æˆªæ–­å¤„ç† (ä¿æŒä¸å˜) ...
        conservative_base_len = (self.max_token_length - 1) * 6 + 512
        if seq_len > conservative_base_len:
            sequence = sequence[-conservative_base_len:]
            seq_len = len(sequence)
        
        # ==================== 2. åˆ›å»º base-level mask (æ ¸å¿ƒä¿®æ”¹) ====================
        try:
            motif_coords = self.motif_data.loc[unique_id]
            # ä¼ å…¥ is_training æ ‡å¿—ä»¥å¯ç”¨åŠ¨æ€æƒé‡
            base_mask = self._create_base_level_mask(seq_len, motif_coords)
        except KeyError:
            # é»˜è®¤èƒŒæ™¯å€¼ä¹Ÿç¨å¾®åŠ¨æ€åŒ–
            bg_val = 0.3
            if self.is_training:
                bg_val = np.random.uniform(*self.weight_ranges['background'])
            base_mask = np.full(seq_len, bg_val, dtype=np.float32)
        
        # 3. ç¼–ç  (ä¿æŒä¸å˜) ...
        content_ids = self.tokenizer.encode(sequence, add_special_tokens=False)
        if isinstance(content_ids, torch.Tensor):
            content_ids = content_ids.tolist()
        
        max_content_tokens = self.max_token_length - 1
        if len(content_ids) > max_content_tokens:
            content_ids = content_ids[-max_content_tokens:]
        
        # 4. Token æ˜ å°„ (ä¿æŒä¸å˜) ...
        mapping = []
        pos = 0
        for tid in content_ids:
            tok_str = self.tokenizer.decode([tid])
            tok_len = len(tok_str)
            if tok_str == '<unk>': tok_len = 1
            mapping.append((pos, pos + tok_len))
            pos += tok_len
        
        skipped_bases = seq_len - pos
        if skipped_bases > 0:
            base_mask = base_mask[skipped_bases:]
        
        if len(base_mask) != pos:
            if len(base_mask) > pos:
                base_mask = base_mask[-pos:]
            else:
                pad = np.full(pos - len(base_mask), 0.1, dtype=np.float32)
                base_mask = np.concatenate([pad, base_mask])
        
        # 5. æ„å»º input_ids (ä¿æŒä¸å˜) ...
        input_ids = torch.full((self.max_token_length,), self.pad_token_id, dtype=torch.long)
        attention_mask = torch.zeros(self.max_token_length, dtype=torch.long)
        input_ids[0] = self.cls_token_id
        attention_mask[0] = 1
        content_len = len(content_ids)
        input_ids[1:1+content_len] = torch.tensor(content_ids, dtype=torch.long)
        attention_mask[1:1+content_len] = 1
        
        # 6. æ„å»º token_mask (ä½¿ç”¨ max pooling ä» base æ˜ å°„åˆ° token)
        token_mask = torch.full((self.max_token_length,), 0.0, dtype=torch.float32)
        token_mask[0] = float(base_mask.mean()) if len(base_mask) > 0 else 0.3
        
        for i, (start, end) in enumerate(mapping):
            token_idx = i + 1
            segment = base_mask[start:end]
            if len(segment) > 0:
                token_mask[token_idx] = float(np.max(segment))
            else:
                token_mask[token_idx] = 0.3

        # ==================== 7. Mask Dropout (ä¿®æ”¹ä¸º 0.7) ====================
        if self.is_training:
            rand_val = np.random.rand()
            
            # [ä¿®æ”¹ç‚¹] å°†æ¦‚ç‡é˜ˆå€¼æé«˜åˆ° 0.7
            # 70% çš„æ¦‚ç‡æŠ¹å¹³ Maskï¼Œè¿«ä½¿æ¨¡å‹çœ‹åºåˆ—
            if rand_val < 0.7:
                # ä½¿ç”¨å½“å‰çš„èƒŒæ™¯å€¼èŒƒå›´å‡å€¼ä½œä¸ºâ€œå¹³å¦â€å€¼
                flat_val = sum(self.weight_ranges['background']) / 2
                
                # è·å–é padding åŒºåŸŸ
                is_padding = (input_ids == self.pad_token_id)
                token_mask.fill_(flat_val)
                token_mask[is_padding] = 0.0
                
            # å¦å¤– 30% çš„æ¦‚ç‡ï¼Œä¿ç•™ Mask (ä½†å·²ç»åœ¨ _create_base_level_mask é‡ŒåŠ å…¥äº†æŠ–åŠ¨å’Œå¹³æ»‘)
            # è¿™é‡Œå¯ä»¥é¢å¤–å†åŠ ä¸€ç‚¹ç‚¹é«˜æ–¯å™ªå£°ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆç²¾ç¡®çš„æµ®ç‚¹æ•°
            else:
                noise = torch.randn_like(token_mask) * 0.1
                token_mask = (token_mask + noise).clamp(min=0.1, max=3.0)
                is_padding = (input_ids == self.pad_token_id)
                token_mask[is_padding] = 0.0

        result = {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'motif_mask': token_mask,
            'unique_id': unique_id
        }
        
        if self.is_training:
            result['label'] = torch.tensor(label, dtype=torch.long)
        
        return result

    def _create_base_level_mask(self, seq_len: int, motif_coords: pd.Series) -> np.ndarray:
        """
        åˆ›å»ºå¸¦æœ‰åŠ¨æ€æƒé‡å’Œé«˜æ–¯å¹³æ»‘çš„ Mask
        """
        # 1. ç¡®å®šèƒŒæ™¯å€¼
        if self.is_training:
            bg_val = np.random.uniform(*self.weight_ranges['background'])
        else:
            bg_val = sum(self.weight_ranges['background']) / 2
            
        mask = np.full(seq_len, bg_val, dtype=np.float32)
        
        # 2. ç¡®å®š Motif æƒé‡
        original_sine_start = int(motif_coords.get('original_sine_start_rel', 0))
        
        feature_keys = ['A_box', 'B_box', 'polyA', 'left_TSD', 'right_TSD']
        
        for feature in feature_keys:
            start = motif_coords.get(f'{feature}_start', -1)
            end = motif_coords.get(f'{feature}_end', -1)
            
            if pd.isna(start) or start == -1:
                continue
                
            start, end = int(start), int(end)
            
            # è·å–æƒé‡
            if self.is_training:
                weight = np.random.uniform(*self.weight_ranges[feature])
            else:
                weight = sum(self.weight_ranges[feature]) / 2
            
            # æ˜ å°„åæ ‡
            rel_start = max(0, start - original_sine_start)
            rel_end = min(seq_len, end - original_sine_start)
            
            if rel_start < rel_end:
                # ä½¿ç”¨ maximum å åŠ æƒé‡ (æ¯”å¦‚ TSD å’Œ A-box é‡å æ—¶ï¼Œå–å¤§å€¼)
                mask[rel_start:rel_end] = np.maximum(mask[rel_start:rel_end], weight)
        
        # 3. åº”ç”¨é«˜æ–¯å¹³æ»‘ (Soft Masking)
        # Sigma å€¼ä¹Ÿå¯ä»¥å¾®è°ƒï¼šè®­ç»ƒæ—¶ç¨å¾®å¤§ä¸€ç‚¹å¢åŠ æ¨¡ç³Šåº¦ï¼Œé¢„æµ‹æ—¶æ ‡å‡†ä¸€ç‚¹
        sigma = 3.0 if self.is_training else 2.0
        mask = gaussian_kernel_smooth(mask, sigma=sigma)
        
        return mask


def collate_fn(batch):
    input_ids = torch.stack([item['input_ids'] for item in batch])
    attention_mask = torch.stack([item['attention_mask'] for item in batch])
    motif_mask = torch.stack([item['motif_mask'] for item in batch])
    unique_ids = [item['unique_id'] for item in batch]
    
    result = {
        'input_ids': input_ids,
        'attention_mask': attention_mask,
        'motif_mask': motif_mask,
        'unique_ids': unique_ids
    }
    
    if 'label' in batch[0]:
        result['label'] = torch.stack([item['label'] for item in batch])
    
    return result



def print_batch_info(batch):
    """æ‰“å°batchä¿¡æ¯ç”¨äºè°ƒè¯•"""
    print("\nBatchä¿¡æ¯:")
    print(f"  Batch size: {batch['input_ids'].size(0)}")
    print(f"  Sequence length: {batch['input_ids'].size(1)}")
    print(f"  input_ids shape: {batch['input_ids'].shape}")
    print(f"  attention_mask shape: {batch['attention_mask'].shape}")
    print(f"  motif_mask shape: {batch['motif_mask'].shape}")
    
    if 'label' in batch:
        print(f"  labels: {batch['label']}")
        print(f"  SINEæ¯”ä¾‹: {batch['label'].float().mean().item():.2%}")


def visualize_token_alignment(dataset, idx=0):
    """
    å¯è§†åŒ–tokenä¸motif maskçš„å¯¹é½æƒ…å†µ
    """
    sample = dataset[idx]
    tokenizer = dataset.tokenizer
    
    input_ids = sample['input_ids']
    motif_mask = sample['motif_mask']
    
    print("\n" + "="*80)
    print("Token-Motif Alignment Visualization")
    print("="*80)
    
    print(f"\nSample ID: {sample.get('unique_id', 'N/A')}")
    print(f"Total tokens: {len(input_ids)}")
    print(f"High-weight tokens (>1.5): {(motif_mask > 1.5).sum()}")
    print(f"Mid-weight tokens (0.8-1.5): {((motif_mask > 0.8) & (motif_mask <= 1.5)).sum()}")
    
    print("\nToken details (å‰30ä¸ª):")
    print(f"{'Pos':<5} {'Token':<15} {'Mask':<12} {'Type'}")
    print("-" * 80)
    
    for i, (token_id, mask_val) in enumerate(zip(input_ids, motif_mask)):
        token_str = tokenizer.decode([token_id])
        
        # åˆ†ç±»æƒé‡
        if mask_val > 1.8:
            comment = "ğŸ”´ A/B-box"
        elif mask_val > 1.3:
            comment = "ğŸŸ¡ PolyA"
        elif mask_val > 0.8:
            comment = "ğŸŸ¢ TSD"
        elif mask_val > 0.5:
            comment = "âšª Background"
        else:
            comment = "âš« Padding"
        
        print(f"{i:<5} {token_str:<15} {mask_val:<12.2f} {comment}")
        
        if i >= 29:
            print("... (truncated)")
            break
    
    print("="*80 + "\n")

