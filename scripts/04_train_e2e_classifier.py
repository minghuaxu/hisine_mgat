#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
04_train_e2e_classifier.py
==========================
çœŸæ­£çš„ç«¯åˆ°ç«¯SINEåˆ†ç±»å™¨è®­ç»ƒ

å…³é”®ä¿®å¤:
1. âŒ ä¸ä½¿ç”¨é¢„è®¡ç®—çš„embedding
2. âœ… æ¯ä¸ªbatchåŠ¨æ€æå–ç‰¹å¾
3. âœ… Backboneå‚ä¸è®­ç»ƒï¼Œæƒé‡ä¼šæ›´æ–°
4. âœ… æ¢¯åº¦å¯ä»¥åå‘ä¼ æ’­åˆ°Backbone
"""

import argparse
import logging
import os
from pathlib import Path

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import DataLoader
from torch.utils.data.distributed import DistributedSampler
from transformers import AutoModelForMaskedLM, AutoTokenizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score
from tqdm import tqdm
from Bio import SeqIO

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
import sys
sys.path.insert(0, str(Path(__file__).parent.parent))
from sine_classifier.data import SINEDatasetE2E, collate_fn
from sine_classifier.model import MotifGuidedSINEClassifier, FocalLoss

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def setup_ddp():
    """åˆå§‹åŒ–DDP"""
    dist.init_process_group(backend='nccl')
    local_rank = int(os.environ["LOCAL_RANK"])
    torch.cuda.set_device(local_rank)
    return local_rank, int(os.environ["WORLD_SIZE"])


def cleanup_ddp():
    """æ¸…ç†DDP"""
    dist.destroy_process_group()


def load_sine_data(fasta_file: str):
    """
    ä»FASTAåŠ è½½æ•°æ®
    
    æ ¼å¼: >unique_id_LABEL
    """
    sequences_with_ids = []
    labels = []
    label_mapping = {'SINE': 1, 'nonSINE': 0}
    
    for record in SeqIO.parse(fasta_file, "fasta"):
        try:
            unique_id, label_name = record.id.rsplit('_', 1)
            if label_name in label_mapping:
                sequences_with_ids.append((unique_id, str(record.seq).upper()))
                labels.append(label_mapping[label_name])
        except ValueError:
            logger.warning(f"æ— æ³•è§£ææ ‡ç­¾: {record.id}")
    
    return sequences_with_ids, labels


def train_epoch(
    model, 
    loader, 
    criterion, 
    optimizer, 
    device, 
    epoch, 
    rank, 
    world_size
):
    """
    è®­ç»ƒä¸€ä¸ªepoch
    
    å…³é”®: æ¯ä¸ªbatchä¸­ï¼Œåºåˆ—ä¼šè¢«é€å…¥backboneåŠ¨æ€æå–ç‰¹å¾
    """
    model.train()
    
    if hasattr(loader.sampler, 'set_epoch'):
        loader.sampler.set_epoch(epoch)
    
    total_loss = 0
    all_preds = []
    all_labels = []
    
    iterator = tqdm(loader, desc=f"Epoch {epoch}", disable=(rank != 0))
    
    for batch_idx, batch in enumerate(iterator):
        # å°†æ•°æ®ç§»åˆ°GPU
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        motif_mask = batch['motif_mask'].to(device)
        labels = batch['label'].to(device)
        
        # æ¸…é›¶æ¢¯åº¦
        optimizer.zero_grad()
        
        # å‰å‘ä¼ æ’­ï¼ˆå…³é”®ï¼šbackboneä¼šåœ¨è¿™é‡ŒåŠ¨æ€æå–ç‰¹å¾ï¼‰
        logits = model(input_ids, attention_mask, motif_mask)
        loss = criterion(logits, labels)
        
        # åå‘ä¼ æ’­ï¼ˆæ¢¯åº¦ä¼šä¼ åˆ°backboneï¼‰
        loss.backward()
        
        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        
        # æ›´æ–°å‚æ•°ï¼ˆåŒ…æ‹¬backboneçš„å‚æ•°ï¼‰
        optimizer.step()
        
        # è®°å½•
        total_loss += loss.item()
        preds = torch.argmax(logits, dim=1)
        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())
        
        if rank == 0:
            iterator.set_postfix({'loss': f'{loss.item():.12f}'})
    
    # åŒæ­¥æŒ‡æ ‡
    avg_loss = total_loss / len(loader)
    acc = accuracy_score(all_labels, all_preds)
    
    if world_size > 1:
        metrics = torch.tensor([avg_loss, acc], device=device)
        dist.all_reduce(metrics, op=dist.ReduceOp.AVG)
        avg_loss, acc = metrics.cpu().numpy()
    
    return float(avg_loss), float(acc)


@torch.no_grad()
def evaluate(model, loader, criterion, device, rank):
    """è¯„ä¼°æ¨¡å‹"""
    model.eval()
    
    total_loss = 0
    all_preds = []
    all_labels = []
    all_probs = []
    
    iterator = tqdm(loader, desc="Evaluating", disable=(rank != 0))
    
    for batch in iterator:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        motif_mask = batch['motif_mask'].to(device)
        labels = batch['label'].to(device)
        
        # å‰å‘ä¼ æ’­ï¼ˆbackboneä»ä¼šæå–ç‰¹å¾ï¼Œä½†ä¸è®¡ç®—æ¢¯åº¦ï¼‰
        logits = model(input_ids, attention_mask, motif_mask)
        loss = criterion(logits, labels)
        
        total_loss += loss.item()
        
        probs = torch.softmax(logits, dim=1)
        preds = torch.argmax(logits, dim=1)
        
        all_probs.append(probs.cpu().numpy())
        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())
    
    all_probs = np.vstack(all_probs)
    
    # è®¡ç®—æŒ‡æ ‡
    avg_loss = total_loss / len(loader)
    acc = accuracy_score(all_labels, all_preds)
    precision, recall, f1, _ = precision_recall_fscore_support(
        all_labels, all_preds, average='weighted'
    )
    
    try:
        auc = roc_auc_score(all_labels, all_probs[:, 1])
    except ValueError:
        auc = 0.0
    
    return avg_loss, acc, f1, auc


def main():
    parser = argparse.ArgumentParser(
        description="ç«¯åˆ°ç«¯SINEåˆ†ç±»å™¨è®­ç»ƒï¼ˆæ­£ç¡®å®ç°ï¼‰"
    )
    
    # æ•°æ®å‚æ•°
    parser.add_argument("--backbone_path", required=True, help="Plant-NT backboneè·¯å¾„")
    parser.add_argument("--sine_data_path", required=True, help="è®­ç»ƒæ•°æ®FASTA")
    parser.add_argument("--motif_data_path", required=True, help="Motifåæ ‡TSV")
    parser.add_argument("--output_dir", required=True, help="è¾“å‡ºç›®å½•")
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument("--epochs", type=int, default=30, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--batch_size", type=int, default=16, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--backbone_lr", type=float, default=1e-5, help="Backboneå­¦ä¹ ç‡")
    parser.add_argument("--head_lr", type=float, default=1e-4, help="åˆ†ç±»å¤´å­¦ä¹ ç‡")
    parser.add_argument("--hidden_dim", type=int, default=256, help="éšè—å±‚ç»´åº¦")
    parser.add_argument("--dropout", type=float, default=0.1, help="Dropoutç‡")
    parser.add_argument("--max_length", type=int, default=512, help="æœ€å¤§åºåˆ—é•¿åº¦")
    parser.add_argument("--freeze_backbone", action='store_true', help="å†»ç»“backbone")
    
    parser.add_argument("--split_dir", type=str, default=None, 
                        help="åŒ…å« train_ids.txt å’Œ val_ids.txt çš„ç›®å½•ã€‚å¦‚æœæœªæä¾›ï¼Œåˆ™ä½¿ç”¨éšæœºåˆ’åˆ†ï¼ˆä¸æ¨èï¼‰ã€‚")
    args = parser.parse_args()
    
    # DDPåˆå§‹åŒ–
    rank, world_size = setup_ddp()
    device = torch.device(f'cuda:{rank}')
    
    if rank == 0:
        logger.info("="*80)
        logger.info("ç«¯åˆ°ç«¯SINEåˆ†ç±»å™¨è®­ç»ƒ")
        logger.info("="*80)
        logger.info(f"ä½¿ç”¨{world_size}ä¸ªGPUè¿›è¡ŒDDPè®­ç»ƒ")
        logger.info(f"Backboneå­¦ä¹ ç‡: {args.backbone_lr}")
        logger.info(f"åˆ†ç±»å¤´å­¦ä¹ ç‡: {args.head_lr}")
        logger.info(f"Backboneå†»ç»“: {args.freeze_backbone}")
        logger.info("="*80)
        
        output_dir = Path(args.output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
    
    # åŠ è½½tokenizerå’Œbackbone
    if rank == 0:
        logger.info("\nåŠ è½½æ¨¡å‹å’Œtokenizer...")
    
    tokenizer = AutoTokenizer.from_pretrained(
        args.backbone_path, 
        trust_remote_code=True
    )
    backbone = AutoModelForMaskedLM.from_pretrained(
        args.backbone_path, 
        trust_remote_code=True
    )
    
    # åˆ›å»ºåˆ†ç±»å™¨
    model = MotifGuidedSINEClassifier(
        backbone=backbone,
        hidden_dim=args.hidden_dim,
        num_classes=2,
        dropout=args.dropout,
        freeze_backbone=args.freeze_backbone  # æ ¹æ®å‚æ•°å†³å®šæ˜¯å¦å†»ç»“
    ).to(device)
    
    # DDPåŒ…è£…
    model = DDP(model, device_ids=[rank], find_unused_parameters=True)
    
    if rank == 0:
        # ç»Ÿè®¡å‚æ•°
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        logger.info(f"\næ¨¡å‹å‚æ•°:")
        logger.info(f"  æ€»å‚æ•°: {total_params:,}")
        logger.info(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,} ({100*trainable_params/total_params:.1f}%)")
    
    # åŠ è½½æ•°æ®
    if rank == 0:
        logger.info("\nåŠ è½½è®­ç»ƒæ•°æ®...")
    
    sequences_with_ids, labels = load_sine_data(args.sine_data_path)
    
    if rank == 0:
        logger.info(f"  æ€»æ ·æœ¬æ•°: {len(sequences_with_ids)}")
        logger.info(f"  SINEæ ·æœ¬: {sum(labels)}")
        logger.info(f"  nonSINEæ ·æœ¬: {len(labels) - sum(labels)}")
    
    motif_df = pd.read_csv(args.motif_data_path, sep='\t')
    
    # åˆ›å»ºunique_idåˆ—ï¼ˆå¦‚æœæ²¡æœ‰ï¼‰
    if 'unique_id' not in motif_df.columns:
        motif_df['unique_id'] = motif_df.apply(
            lambda row: f"{row['chrom']}:{row['original_start']}-{row['original_end']}({row['strand']})",
            axis=1
        )
    motif_df.drop_duplicates(subset=['unique_id'], keep='first', inplace=True)
    
    # æ•°æ®é›†åˆ’åˆ†
    # X_train, X_val, y_train, y_val = train_test_split(
    #     sequences_with_ids, labels,
    #     test_size=0.2,
    #     random_state=42,
    #     stratify=labels
    # )

    X_train, X_val, y_train, y_val = [], [], [], []

    if args.split_dir and os.path.exists(args.split_dir):
        if rank == 0:
            logger.info(f"ä½¿ç”¨é¢„å®šä¹‰çš„ç°‡åˆ’åˆ† (CD-HIT): {args.split_dir}")
        
        train_id_file = os.path.join(args.split_dir, "train_ids.txt")
        val_id_file = os.path.join(args.split_dir, "val_ids.txt")
        
        # è¯»å– ID é›†åˆ
        def load_ids_set(file_path):
            ids = set()
            with open(file_path, 'r') as f:
                for line in f:
                    line = line.strip()
                    if not line: continue
                    
                    # å…³é”®ä¿®å¤: éœ€è¦ä¸ load_sine_data çš„é€»è¾‘ä¿æŒä¸€è‡´
                    # å°† 'ID_LABEL' æ ¼å¼åˆ†å‰²ä¸º 'ID'ï¼Œå»æ‰åç¼€
                    try:
                        uid, _ = line.rsplit('_', 1)
                        ids.add(uid)
                    except ValueError:
                        # å¦‚æœæ²¡æœ‰ä¸‹åˆ’çº¿ï¼Œåˆ™å‡è®¾æ•´è¡Œå°±æ˜¯ ID
                        ids.add(line)
            return ids
            
        # è¯»å– ID é›†åˆ
        train_ids_set = load_ids_set(train_id_file)
        val_ids_set = load_ids_set(val_id_file)
            
        # æ„å»ºæŸ¥æ‰¾å­—å…¸
        # ä¸ºäº†åŠ å¿«é€Ÿåº¦ï¼Œå…ˆæŠŠ data è½¬æ¢æˆ dict: uid -> (seq, label)
        # æ³¨æ„ load_sine_data è¿”å›çš„æ˜¯ [(uid, seq), ...], labels æ˜¯ [label, ...]
        data_map = {}
        for (uid, seq), lbl in zip(sequences_with_ids, labels):
            data_map[uid] = (uid, seq, lbl)
            
        # å¡«å……è®­ç»ƒé›†
        for uid in train_ids_set:
            if uid in data_map:
                u, s, l = data_map[uid]
                X_train.append((u, s))
                y_train.append(l)
        
        # å¡«å……éªŒè¯é›†
        for uid in val_ids_set:
            if uid in data_map:
                u, s, l = data_map[uid]
                X_val.append((u, s))
                y_val.append(l)
                
    else:
        if rank == 0:
            logger.warning("æœªæ‰¾åˆ°åˆ’åˆ†æ–‡ä»¶ï¼Œä½¿ç”¨éšæœºåˆ’åˆ† (è­¦å‘Š: å¯èƒ½å­˜åœ¨æ•°æ®æ³„éœ²!)")
        # å›é€€åˆ°éšæœºåˆ’åˆ†
        X_train, X_val, y_train, y_val = train_test_split(
            sequences_with_ids, labels,
            test_size=0.2,
            random_state=42,
            stratify=labels
        )

    if rank == 0:
        logger.info(f"\næ•°æ®é›†åˆ’åˆ†å®Œæˆ:")
        logger.info(f"  è®­ç»ƒé›†: {len(X_train)}")
        logger.info(f"  éªŒè¯é›†: {len(X_val)}")
        if len(X_train) == 0 or len(X_val) == 0:
            logger.error("é”™è¯¯: è®­ç»ƒé›†æˆ–éªŒè¯é›†ä¸ºç©ºï¼è¯·æ£€æŸ¥ ID åŒ¹é…æƒ…å†µã€‚")
            sys.exit(1)
    
    # åˆ›å»ºDatasetï¼ˆä¸é¢„è®¡ç®—embeddingï¼‰
    train_dataset = SINEDatasetE2E(
        X_train, y_train, motif_df, tokenizer, args.max_length
    )
    val_dataset = SINEDatasetE2E(
        X_val, y_val, motif_df, tokenizer, args.max_length
    )
    
    # DataLoader with DDP
    train_sampler = DistributedSampler(
        train_dataset,
        num_replicas=world_size,
        rank=rank,
        shuffle=True
    )
    val_sampler = DistributedSampler(
        val_dataset,
        num_replicas=world_size,
        rank=rank,
        shuffle=False
    )
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        sampler=train_sampler,
        num_workers=4,
        pin_memory=True,
        collate_fn=collate_fn
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=args.batch_size,
        sampler=val_sampler,
        num_workers=4,
        pin_memory=True,
        collate_fn=collate_fn
    )
    
    # ä¼˜åŒ–å™¨ï¼ˆæ ¹æ®æ˜¯å¦å†»ç»“backboneè°ƒæ•´ï¼‰
    if args.freeze_backbone:
        # åªä¼˜åŒ–åˆ†ç±»å¤´
        optimizer = torch.optim.AdamW(
            [p for p in model.parameters() if p.requires_grad],
            lr=args.head_lr,
            weight_decay=0.01
        )
        if rank == 0:
            logger.info("\nä¼˜åŒ–å™¨: ä»…è®­ç»ƒåˆ†ç±»å¤´")
    else:
        # åˆ†åˆ«è®¾ç½®backboneå’Œåˆ†ç±»å¤´çš„å­¦ä¹ ç‡
        optimizer = torch.optim.AdamW([
            {'params': model.module.backbone.parameters(), 'lr': args.backbone_lr},
            {'params': model.module.motif_attention.parameters(), 'lr': args.head_lr},
            {'params': model.module.classifier.parameters(), 'lr': args.head_lr}
        ], weight_decay=0.01)
        if rank == 0:
            logger.info("\nä¼˜åŒ–å™¨: Backbone + åˆ†ç±»å¤´è”åˆè®­ç»ƒ")
            logger.info(f"  Backbone LR: {args.backbone_lr}")
            logger.info(f"  Head LR: {args.head_lr}")
    
    # criterion = nn.CrossEntropyLoss()
    class_weights = torch.tensor([3.0, 1.0]).to(device)
    criterion = nn.CrossEntropyLoss(weight=class_weights)
    
    # è®­ç»ƒå¾ªç¯
    best_val_f1 = 0.0
    
    if rank == 0:
        logger.info("\nå¼€å§‹è®­ç»ƒ...")
        logger.info("="*80)
    
    for epoch in range(1, args.epochs + 1):
        # è®­ç»ƒ
        train_loss, train_acc = train_epoch(
            model, train_loader, criterion, optimizer,
            device, epoch, rank, world_size
        )
        
        # éªŒè¯
        val_loss, val_acc, val_f1, val_auc = evaluate(
            model.module, val_loader, criterion, device, rank
        )
        
        if rank == 0:
            logger.info(
                f"Epoch {epoch:2d}/{args.epochs} | "
                f"Train: Loss={train_loss:.20f} Acc={train_acc:.4f} | "
                f"Val: Loss={val_loss:.20f} Acc={val_acc:.4f} F1={val_f1:.4f} AUC={val_auc:.4f}"
            )
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_f1 > best_val_f1:
                best_val_f1 = val_f1
                save_path = output_dir / "best_model.pt"
                torch.save(model.module.state_dict(), save_path)
                logger.info(f"  âœ… ä¿å­˜æœ€ä½³æ¨¡å‹ (F1: {val_f1:.4f})")

            checkpoint_data = {
                'epoch': epoch,
                'model_state_dict': model.module.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'train_loss': train_loss,
                'val_f1': val_f1,
            }

            history_ckpt_path = output_dir / f"checkpoint_epoch_{epoch}.pt"
            torch.save(checkpoint_data, history_ckpt_path)

            # 4. æ¯è½®éƒ½è¦†ç›–ä¿å­˜æœ€æ–°çš„æ£€æŸ¥ç‚¹ (latest.pt) - æ–¹ä¾¿Snakemakeå¤±è´¥åæ‰‹åŠ¨æ¢å¤
            latest_ckpt_path = output_dir / "latest.pt"
            torch.save(checkpoint_data, latest_ckpt_path)

            logger.info(f"  ğŸ’¾ å·²ä¿å­˜æ£€æŸ¥ç‚¹: {history_ckpt_path.name}")
    
    if rank == 0:
        logger.info("="*80)
        logger.info(f"è®­ç»ƒå®Œæˆ! æœ€ä½³F1: {best_val_f1:.4f}")
        logger.info("="*80)
    
    cleanup_ddp()


if __name__ == "__main__":
    main()