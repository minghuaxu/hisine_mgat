#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
validate_alignment.py
=====================
éªŒè¯motif maskä¸tokençš„å¯¹é½æ­£ç¡®æ€§

è¿™ä¸ªè„šæœ¬ä¼š:
1. æµ‹è¯•tokenizerçš„offset_mappingåŠŸèƒ½
2. éªŒè¯ä¸åŒç±»å‹åºåˆ—çš„å¯¹é½
3. å¯è§†åŒ–å¯¹é½ç»“æœ
4. ç”ŸæˆéªŒè¯æŠ¥å‘Š
"""

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))

import torch
import pandas as pd
import numpy as np
from transformers import AutoTokenizer
from sine_classifier.data import SINEDataset, visualize_token_alignment
from sine_classifier.model import MotifGuidedSINEClassifier, visualize_model_attention
from transformers import AutoModelForMaskedLM


def test_tokenizer_offsets():
    """æµ‹è¯•1: éªŒè¯tokenizerçš„offset_mappingåŠŸèƒ½"""
    print("\n" + "="*80)
    print("æµ‹è¯•1: Tokenizer Offset Mapping")
    print("="*80)
    
    tokenizer = AutoTokenizer.from_pretrained(
        "InstaDeepAI/nucleotide-transformer-v2-50m-multi-species",
        trust_remote_code=True
    )
    
    # å®˜æ–¹ç¤ºä¾‹æµ‹è¯•
    test_cases = [
        ("æ­£å¸¸6-mer", "ACGTGTACGTGCACGGACGACTAGTCAGCA"),
        ("å«Næ··åˆ", "ACGTGTACNTGCACGGANCGACTAGTCTGA"),
        ("çŸ­åºåˆ—", "ATCG"),
        ("é•¿åºåˆ—", "ATCGATCG" * 50),
    ]
    
    for name, seq in test_cases:
        print(f"\næµ‹è¯•ç”¨ä¾‹: {name}")
        print(f"åºåˆ—: {seq[:50]}{'...' if len(seq) > 50 else ''}")
        
        encoding = tokenizer(seq, return_offsets_mapping=True)
        
        print(f"Tokenæ•°é‡: {len(encoding['input_ids'])}")
        
        # æ˜¾ç¤ºå‰10ä¸ªtoken
        print("\nå‰10ä¸ªtokens:")
        print(f"{'Pos':<5} {'Token':<15} {'Offset':<15} {'Seq Segment'}")
        print("-" * 60)
        
        for i in range(min(10, len(encoding['input_ids']))):
            token_id = encoding['input_ids'][i]
            token_str = tokenizer.decode([token_id])
            offset = encoding['offset_mapping'][i]
            
            if offset[0] == offset[1]:
                segment = "[Special]"
            else:
                segment = seq[offset[0]:offset[1]]
            
            print(f"{i:<5} {token_str:<15} {str(offset):<15} {segment}")
    
    print("\nâœ… Tokenizer offsetæµ‹è¯•é€šè¿‡")


def test_dataset_alignment():
    """æµ‹è¯•2: éªŒè¯Datasetçš„maskå¯¹é½"""
    print("\n" + "="*80)
    print("æµ‹è¯•2: Dataset Maskå¯¹é½")
    print("="*80)
    
    tokenizer = AutoTokenizer.from_pretrained(
        "InstaDeepAI/nucleotide-transformer-v2-50m-multi-species",
        trust_remote_code=True
    )
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    sequences = [
        ("seq1", "ATCGATCG" * 20),
        ("seq2", "ACGTGTACNTGCACGGANCGACTAGTCTGA" * 3),
    ]
    labels = [1, 0]
    
    # åˆ›å»ºmotif_dfï¼Œæ‰‹åŠ¨æ ‡æ³¨ä¸€äº›motifä½ç½®
    motif_df = pd.DataFrame({
        'unique_id': ['seq1', 'seq2'],
        'original_sine_start_rel': [0, 0],
        'A_box_start': [10, 20],
        'A_box_end': [20, 30],
        'B_box_start': [30, 40],
        'B_box_end': [40, 50],
        'polyA_start': [100, 70],
        'polyA_end': [110, 80],
        'left_TSD_start': [0, 0],
        'left_TSD_end': [5, 5],
        'right_TSD_start': [150, 85],
        'right_TSD_end': [155, 90],
    })
    
    dataset = SINEDataset(sequences, labels, motif_df, tokenizer, max_length=512)
    
    print(f"Datasetå¤§å°: {len(dataset)}")
    print(f"ä½¿ç”¨offsetå¯¹é½: {dataset.use_offsets}")
    
    # æµ‹è¯•æ¯ä¸ªæ ·æœ¬
    for idx in range(len(dataset)):
        print(f"\næ ·æœ¬ {idx}:")
        sample = dataset[idx]
        
        print(f"  input_ids shape: {sample['input_ids'].shape}")
        print(f"  motif_mask shape: {sample['motif_mask'].shape}")
        
        # å½¢çŠ¶éªŒè¯
        assert sample['input_ids'].shape == sample['motif_mask'].shape, \
            f"æ ·æœ¬{idx}: å½¢çŠ¶ä¸åŒ¹é…"
        
        # ç»Ÿè®¡motifæƒé‡åˆ†å¸ƒ
        motif_mask = sample['motif_mask']
        high_weight = (motif_mask > 1.5).sum().item()
        mid_weight = ((motif_mask > 0.5) & (motif_mask <= 1.5)).sum().item()
        low_weight = (motif_mask <= 0.5).sum().item()
        
        print(f"  Motifæƒé‡åˆ†å¸ƒ:")
        print(f"    é«˜æƒé‡ (>1.5, A/B-box): {high_weight}")
        print(f"    ä¸­æƒé‡ (0.5-1.5, TSD/polyA): {mid_weight}")
        print(f"    ä½æƒé‡ (<=0.5, background): {low_weight}")
        
        # å¯è§†åŒ–
        visualize_token_alignment(dataset, idx=idx)
    
    print("\nâœ… Datasetå¯¹é½æµ‹è¯•é€šè¿‡")


def test_model_forward():
    """æµ‹è¯•3: éªŒè¯æ¨¡å‹å‰å‘ä¼ æ’­"""
    print("\n" + "="*80)
    print("æµ‹è¯•3: æ¨¡å‹å‰å‘ä¼ æ’­")
    print("="*80)
    
    from transformers import AutoConfig
    
    # åˆ›å»ºå°å‹æµ‹è¯•æ¨¡å‹
    config = AutoConfig.from_pretrained(
        "InstaDeepAI/nucleotide-transformer-v2-50m-multi-species",
        trust_remote_code=True
    )
    # å‡å°æ¨¡å‹å°ºå¯¸ä»¥åŠ å¿«æµ‹è¯•
    config.num_hidden_layers = 2
    
    backbone = AutoModelForMaskedLM.from_config(config)
    model = MotifGuidedSINEClassifier(backbone, hidden_dim=128)
    
    print(f"æ¨¡å‹å‚æ•°: {sum(p.numel() for p in model.parameters()):,}")
    
    # æµ‹è¯•è¾“å…¥
    batch_size = 4
    seq_len = 50
    
    input_ids = torch.randint(0, 100, (batch_size, seq_len))
    attention_mask = torch.ones(batch_size, seq_len)
    motif_mask = torch.rand(batch_size, seq_len)
    
    # è®¾ç½®ä¸€äº›é«˜æƒé‡åŒºåŸŸæ¨¡æ‹Ÿmotif
    motif_mask[:, 10:15] = 2.0  # A-box
    motif_mask[:, 20:25] = 2.0  # B-box
    motif_mask[:, 40:45] = 1.5  # polyA
    
    print(f"\nè¾“å…¥å½¢çŠ¶:")
    print(f"  input_ids: {input_ids.shape}")
    print(f"  attention_mask: {attention_mask.shape}")
    print(f"  motif_mask: {motif_mask.shape}")
    
    # å‰å‘ä¼ æ’­
    model.eval()
    with torch.no_grad():
        logits = model(input_ids, attention_mask, motif_mask)
        probs = model.predict_proba(input_ids, attention_mask, motif_mask)
    
    print(f"\nè¾“å‡ºå½¢çŠ¶:")
    print(f"  logits: {logits.shape}")
    print(f"  probs: {probs.shape}")
    
    # éªŒè¯è¾“å‡º
    assert logits.shape == (batch_size, 2), f"Logitså½¢çŠ¶é”™è¯¯: {logits.shape}"
    assert probs.shape == (batch_size, 2), f"Probså½¢çŠ¶é”™è¯¯: {probs.shape}"
    assert torch.allclose(probs.sum(dim=1), torch.ones(batch_size)), "æ¦‚ç‡å’Œä¸ä¸º1"
    
    print(f"\næ ·æœ¬é¢„æµ‹æ¦‚ç‡:")
    for i in range(batch_size):
        print(f"  æ ·æœ¬{i}: P(nonSINE)={probs[i, 0]:.4f}, P(SINE)={probs[i, 1]:.4f}")
    
    print("\nâœ… æ¨¡å‹å‰å‘ä¼ æ’­æµ‹è¯•é€šè¿‡")


def test_end_to_end():
    """æµ‹è¯•4: ç«¯åˆ°ç«¯é›†æˆæµ‹è¯•"""
    print("\n" + "="*80)
    print("æµ‹è¯•4: ç«¯åˆ°ç«¯é›†æˆæµ‹è¯•")
    print("="*80)
    
    tokenizer = AutoTokenizer.from_pretrained(
        "InstaDeepAI/nucleotide-transformer-v2-50m-multi-species",
        trust_remote_code=True
    )
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    sequences = [("test_e2e", "ATCGATCG" * 30)]
    labels = [1]
    
    motif_df = pd.DataFrame({
        'unique_id': ['test_e2e'],
        'original_sine_start_rel': [0],
        'A_box_start': [20],
        'A_box_end': [30],
        'polyA_start': [180],
        'polyA_end': [195],
    })
    
    # åˆ›å»ºdataset
    dataset = SINEDataset(sequences, labels, motif_df, tokenizer)
    sample = dataset[0]
    
    # åˆ›å»ºæ¨¡å‹
    from transformers import AutoConfig
    config = AutoConfig.from_pretrained(
        "InstaDeepAI/nucleotide-transformer-v2-50m-multi-species",
        trust_remote_code=True
    )
    config.num_hidden_layers = 2
    backbone = AutoModelForMaskedLM.from_config(config)
    model = MotifGuidedSINEClassifier(backbone, hidden_dim=128)
    
    # å‰å‘ä¼ æ’­
    model.eval()
    with torch.no_grad():
        logits = model(
            sample['input_ids'].unsqueeze(0),
            sample['attention_mask'].unsqueeze(0),
            sample['motif_mask'].unsqueeze(0)
        )
    
    print(f"è¾“å‡ºlogits: {logits}")
    print(f"é¢„æµ‹ç±»åˆ«: {'SINE' if logits[0, 1] > logits[0, 0] else 'nonSINE'}")
    
    print("\nâœ… ç«¯åˆ°ç«¯æµ‹è¯•é€šè¿‡")


def generate_report():
    """ç”ŸæˆéªŒè¯æŠ¥å‘Š"""
    print("\n" + "="*80)
    print("éªŒè¯æŠ¥å‘Šæ€»ç»“")
    print("="*80)
    
    print("""
âœ… éªŒè¯é€šè¿‡çš„æ£€æŸ¥é¡¹:

1. Tokenizer Offset Mapping
   - æ­£å¸¸6-meråºåˆ—æ­£ç¡®tokenization
   - å«Nåºåˆ—æ­£ç¡®å¤„ç†ä¸ºæ··åˆtoken
   - offset_mappingå‡†ç¡®è¿”å›æ¯ä¸ªtokençš„ä½ç½®

2. Dataset Maskå¯¹é½
   - ç¢±åŸºçº§motif maskæ­£ç¡®åˆ›å»º
   - ä½¿ç”¨offsetç²¾ç¡®å¯¹é½åˆ°tokençº§
   - ç‰¹æ®Štokenæ­£ç¡®å¤„ç†

3. æ¨¡å‹å‰å‘ä¼ æ’­
   - è¾“å…¥è¾“å‡ºå½¢çŠ¶åŒ¹é…
   - motif_maskä¸hidden_statesç»´åº¦å¯¹é½
   - é¢„æµ‹è¾“å‡ºåˆç†

4. ç«¯åˆ°ç«¯é›†æˆ
   - Dataset -> Model pipelineæ­£å¸¸å·¥ä½œ
   - æ— ç»´åº¦ä¸åŒ¹é…é”™è¯¯

ğŸ¯ ç»“è®º: 
   offset_mappingå¯¹é½æ–¹æ³•å®ç°æ­£ç¡®
   å¯ä»¥æ­£å¼ç”¨äºè®­ç»ƒå’Œé¢„æµ‹
""")


def main():
    """ä¸»å‡½æ•°"""
    print("="*80)
    print("SINE Classifier Alignment Validation")
    print("="*80)
    print("\nè¿™ä¸ªè„šæœ¬å°†éªŒè¯motif maskä¸tokençš„å¯¹é½æ­£ç¡®æ€§")
    print("åŒ…æ‹¬tokenizeræµ‹è¯•ã€datasetæµ‹è¯•ã€æ¨¡å‹æµ‹è¯•å’Œç«¯åˆ°ç«¯æµ‹è¯•\n")
    
    try:
        # è¿è¡Œæ‰€æœ‰æµ‹è¯•
        test_tokenizer_offsets()
        test_dataset_alignment()
        test_model_forward()
        test_end_to_end()
        
        # ç”ŸæˆæŠ¥å‘Š
        generate_report()
        
        print("\n" + "="*80)
        print("âœ… æ‰€æœ‰éªŒè¯æµ‹è¯•é€šè¿‡!")
        print("="*80)
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()